{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8caba3f1-e3fa-48f7-92f0-91c3a9dfc161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 19:17:58.615896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 19:17:59.609241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jupyter-tfg2324vqa/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = os.path.abspath(os.path.dirname(current_dir))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from question_recuperation.question_utils import questions_recuperation\n",
    "from image_recuperation.image_utils import calculate_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1498fabd-f3b0-4b56-ab52-78d3750ad079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-tfg2324vqa/.local/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model_vqa = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('stabilityai/stablelm-zephyr-3b')\n",
    "model_text = AutoModelForCausalLM.from_pretrained(\n",
    "    'stabilityai/stablelm-zephyr-3b',\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18014251-c457-4af2-ac83-f5fe6cb8516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='.'\n",
    "dataType='val2014'\n",
    "annFile='%s/annotations/captions_%s.json'%(dataDir,dataType)\n",
    "subtypes=['results', 'totalEval', 'imgEval']\n",
    "[resFile, evalImgsFile, evalFile]= ['%s/results/captions_%s.json'%(dataDir,subtype) for subtype in subtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5478164-e047-4f39-b50f-1cf83785a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('../../COCO/dataset/instances_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb3ea52-8def-4e76-b48e-2219de4d4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_rec = 50 \n",
    "num_questions_rec = 20\n",
    "dataset_name = 'dataset_2'\n",
    "vocab=pd.read_pickle(f'../question_recuperation/embeddings/gte-small_question_embeddings_{dataset_name}.pickle')\n",
    "df = pd.DataFrame(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b501a-5d6c-4151-ad81-264bc9d79b7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Caption generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da27c65-9365-42db-8a0c-cf2110d3d63d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "../description_evaluation/images/girl_dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-tfg2324vqa/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jupyter-tfg2324vqa/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/jupyter-tfg2324vqa/project/question_recuperation/question_utils.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['cluster'] = clusters\n",
      "/home/jupyter-tfg2324vqa/project/question_recuperation/question_utils.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  r['distances'] = distances\n",
      "/home/jupyter-tfg2324vqa/project/question_recuperation/question_utils.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  r['distances'] = distances\n",
      "/home/jupyter-tfg2324vqa/project/question_recuperation/question_utils.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  r['distances'] = distances\n",
      "/home/jupyter-tfg2324vqa/project/question_recuperation/question_utils.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  r['distances'] = distances\n",
      "/home/jupyter-tfg2324vqa/project/question_recuperation/question_utils.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  r['distances'] = distances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions recovered\n",
      "Predicted answer: brown\n",
      "Predicted answer: walking\n",
      "Predicted answer: black\n",
      "Predicted answer: yes\n",
      "Predicted answer: backpack\n",
      "Predicted answer: sidewalk\n",
      "Predicted answer: no\n",
      "Predicted answer: black\n",
      "Answers predicted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Act as an image caption generator.\n",
      "Read pairs of questions and answer.\n",
      "Generate a short description based on the pairs information.\n",
      "Be brief limiting yourdelf to the information you have.\n",
      "Here are the question-answer pairs:\n",
      "Question: What color is the dog? Answer: brown\n",
      "Question: What is the man doing? Answer: walking\n",
      "Question: What color is his luggage? Answer: black\n",
      "Question: Is it sunny? Answer: yes\n",
      "Question: What does the woman have on her back? Answer: backpack\n",
      "Question: Where is the dog? Answer: sidewalk\n",
      "Question: Is the man traveling? Answer: no\n",
      "Question: What color is the suitcase on the right? Answer: black\n",
      "Just answer with the generated caption<|endoftext|>\n",
      "<|assistant|>\n",
      "A brown dog is walking on a sidewalk with a black suitcase on the right and a black backpack on a woman's back. It is sunny and the man is not traveling.<|endoftext|>\n",
      "Caption generated\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "content_template = \"\"\"Act as an image caption generator.\n",
    "Read pairs of questions and answer.\n",
    "Generate a short description based on the pairs information.\n",
    "Be brief limiting yourdelf to the information you have.\n",
    "Here are the question-answer pairs:\n",
    "{pairs}\n",
    "Just answer with the generated caption\"\"\"\n",
    "\n",
    "temp = 0.8\n",
    "n_q = 8\n",
    "\n",
    "\n",
    "numbers = []\n",
    "\n",
    "images_path = '../description_evaluation/images/'\n",
    "test_images_files = [images_path+f+'.jpg' for f in ['girl_dog']]\n",
    "#existing_images_ids = [file_name[:-4] for file_name in existing_images_files]\n",
    "image_captions = []\n",
    "for file in test_images_files:\n",
    "        print('-'*10)\n",
    "        print(file[:-4])\n",
    "        image = Image.open(file)\n",
    "        df_scores = calculate_similarity(image, 'embeddings', 'cos', coco, dataset_name,'densenet')\n",
    "        questions_rec = questions_recuperation(df,df_scores,num_images_rec,num_questions_rec)\n",
    "        print('Questions recovered')\n",
    "\n",
    "        #vqa\n",
    "        questions = questions_rec[:n_q]\n",
    "        answers = {}\n",
    "        for q in questions:\n",
    "            encoding = processor(image, q, return_tensors=\"pt\")\n",
    "            outputs = model_vqa(**encoding)\n",
    "            logits = outputs.logits\n",
    "            idx = logits.argmax(-1).item()\n",
    "            answer = model_vqa.config.id2label[idx]\n",
    "            answers[q] = answer\n",
    "            print(\"Predicted answer:\", answer)\n",
    "        print('Answers predicted')\n",
    "\n",
    "        #caption\n",
    "        #format prompt\n",
    "        questions_answers = \"\"\n",
    "        for key in answers:\n",
    "            questions_answers += \"Question: \" + key + \" Answer: \" + answers[key] + \"\\n\" \n",
    "        content = content_template.format(pairs = questions_answers[:-1])\n",
    "        prompt = [{'role': 'user', 'content': content}]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        #predict caption\n",
    "        tokens = model_text.generate(\n",
    "            inputs.to(model_text.device),\n",
    "            max_new_tokens=1024,\n",
    "            temperature=temp,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        conversation = tokenizer.decode(tokens[0], skip_special_tokens=False)\n",
    "        print(conversation)\n",
    "        print('Caption generated')\n",
    "        print('-'*10)\n",
    "        image_captions.append((file[:-4], conversation))\n",
    "    \n",
    "with open(\"./results/captions_2.txt\", \"a\") as file:\n",
    "    for cap in image_captions:\n",
    "        file.write(f'{cap[0]}\\n')\n",
    "        file.write(f'{cap[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7554f-ef5a-4601-b923-42b203b5820e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch2]",
   "language": "python",
   "name": "conda-env-pytorch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
